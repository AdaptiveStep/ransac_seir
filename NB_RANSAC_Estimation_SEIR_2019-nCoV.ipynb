{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load and Pre-process Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prepare Environment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import unicodedata\n",
    "import datetime\n",
    "from scipy.integrate import solve_ivp\n",
    "from scipy.stats import poisson\n",
    "from scipy.optimize import minimize\n",
    "import json\n",
    "import plotly.graph_objects as go\n",
    "import plotly.express as px"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Constants"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Adjustables\n",
    "# -- keep updating and attempting\n",
    "# own data: 在这里使用自己的数据集（需要配合下面的人口和交通数据）\n",
    "NCOV_DB_FILENAME = \"data/ncov_20200205.csv\"\n",
    "CHECKPOINT_FILENAME = \"checkpoints/seir_fit_result_20200205\"\n",
    "# own population data\n",
    "POPULATION_DB_FILENAME = \"data/ChinaPopulation.csv\"\n",
    "MIGRATION_INDEX_FILENAME = \"data/BaiduMigarationIndex.csv\"\n",
    "MIGRATION_DEST_FILENAME = \"data/BaiduMigarationTo.csv\"\n",
    "\n",
    "OPTIONS = dict(\n",
    "    POPULATION_THRESHOLD=3e6, # popu greater than th to be included in analysis\n",
    "    ALPHA=1/7,\n",
    "    INIT_BETA=0.4,\n",
    "    INIT_GAMMA=0.15,\n",
    "    INIT_INIT_INFECT=50,\n",
    "    RECENT_INLIER_DAYS=5,\n",
    "    SIMULATION_DAYS=90,\n",
    "    RANSAC_SAMPLE_NUM=4,\n",
    "    ROUNDS=1000,\n",
    "    REGULARISE=dict(\n",
    "        small_beta=1/1000,\n",
    "        small_beta_k=1,\n",
    "        large_beta=10,\n",
    "        large_beta_k=1,\n",
    "        small_gamma=1/30,\n",
    "        small_gamma_k=1,\n",
    "        large_gamma=1/2,\n",
    "        large_gamma_k=1,\n",
    "        small_I0=2,\n",
    "        small_I0_k=1,\n",
    "        large_I0=1000,\n",
    "        large_I0_k=1)\n",
    "    )\n",
    "\n",
    "# Factual constants\n",
    "# -- don't change often unless you know what you are doing\n",
    "# Exclude global cases. See discussion in \"Data Source\".\n",
    "GLOBAL_CITIES = [\n",
    "    '美国', '法国', '澳大利亚', '意大利', '德国', '香港',\n",
    "    '韩国', '英国', '日本', '泰国', '日本', '台湾', '新加坡',\n",
    "    '俄罗斯', '芬兰', '加拿大', '马来西亚', '菲律宾', '印度',\n",
    "    '越南',  '阿联酋', '柬埔寨', '斯里兰卡', '尼泊尔'\n",
    "]\n",
    "DAY_LIWENLIANG = np.datetime64(\"2020-01-01\") # t=0, name is a tribute to our fallen hero\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Helper Functions\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Functions to parse infection database\n",
    "def match_place_name(name1, name2):\n",
    "    if name1.find(name2) == 0 or name2.find(name1) == 0:\n",
    "        return True\n",
    "    else:\n",
    "        return False\n",
    "    \n",
    "def get_city_id(db, q):\n",
    "    if not isinstance(q, str):\n",
    "        return None\n",
    "    ids = db.index[db['City']\\\n",
    "        .map(lambda x:match_place_name(x, q))].tolist()\n",
    "    return ids[0] if len(ids) == 1 else None\n",
    "\n",
    "def parse_chinese_datetime(s):\n",
    "    if not isinstance(s, str):\n",
    "        raise ValueError(f\"Unknown Date {s}\")\n",
    "    \n",
    "    i0 = s.find(\"月\")\n",
    "    i1 = s.find(\"日\")\n",
    "    month_s = unicodedata.normalize('NFKD', s[:i0])\\\n",
    "        .encode('ascii','ignore')\\\n",
    "        .decode()\n",
    "    date_s = unicodedata.normalize('NFKD', s[i0 + 1:i1])\\\n",
    "        .encode('ascii','ignore')\\\n",
    "        .decode()\n",
    "    return datetime.datetime(2020, int(month_s), int(date_s))\n",
    "\n",
    "# To save results\n",
    "class NpEncoder(json.JSONEncoder):\n",
    "    def default(self, obj):\n",
    "        if isinstance(obj, np.integer):\n",
    "            return int(obj)\n",
    "        elif isinstance(obj, np.floating):\n",
    "            return float(obj)\n",
    "        elif isinstance(obj, np.ndarray):\n",
    "            return obj.tolist()\n",
    "        else:\n",
    "            return super(NpEncoder, self).default(obj)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Population"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# g_: global variables\n",
    "g_popu_df = pd.read_csv(POPULATION_DB_FILENAME, encoding=\"utf-8\")\n",
    "g_all_city_num = len(g_popu_df) \n",
    "# all cities with population record, we will selection a subset to fit out model\n",
    "# 这里是所有有人口记录的城市，我们会选取一个子集，满足 1人口足够多，2与武汉的交通在被统计的时间段排入前100\n",
    "\n",
    "g_start_loc_id_in_all = get_city_id(g_popu_df, \"武汉\")\n",
    "g_all_popu_vec = np.zeros(g_all_city_num)\n",
    "for i, r in g_popu_df.iterrows():\n",
    "    p = r['Population2018']\n",
    "    if np.isnan(p):\n",
    "        p = r['Population2010']\n",
    "    g_all_popu_vec[i] = p * 1e4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Infection Number"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load infection data\n",
    "df0 = pd.read_csv(NCOV_DB_FILENAME, encoding=\"utf-16\")\n",
    "df0 = df0.rename(columns={\"报道时间\": \"Date\", \n",
    "                         \"新增确诊\": \"Infected.Inc\",\n",
    "                         \"省份\": \"Place.State\",\n",
    "                         \"城市\": \"Place.City\",})\n",
    "\n",
    "df = df0[['Infected.Inc', 'Place.State', 'Place.City']].copy()\n",
    "df['Date'] = pd.Series(\n",
    "    df0['Date'].map(parse_chinese_datetime),\n",
    "    index=df.index)\n",
    "df['Day.Delta'] = np.round(\n",
    "    (df['Date'] - DAY_LIWENLIANG) / np.timedelta64(1, 'D')).astype(int)\n",
    "df['Place.Id'] = pd.Series([-1] * len(df), index=df.index)\n",
    "df.loc[df['Place.City']==\"恩施土家族苗族自治州\", 'Place.City'] = \"恩施州\"\n",
    "\n",
    "# 1. Some records are at district level, which is not in the city list. \n",
    "# we aggragate those records to cities. E.g. \"通州\" -> \"北京\"\n",
    "# 2. Ignore oversea cases\n",
    "omitted = 0\n",
    "omitted_ind = []\n",
    "verbose = False\n",
    "for index, row in df.iterrows():\n",
    "    p = row['Place.City']\n",
    "    city_id = get_city_id(g_popu_df, p)\n",
    "    if not city_id is None:\n",
    "        df.loc[index, 'Place.Id'] = city_id\n",
    "    else:\n",
    "        p1 = row['Place.State']\n",
    "        city_id1 = get_city_id(g_popu_df, p1)\n",
    "        if not city_id1 is None:\n",
    "            # Aggregate district to megapolitan cities, e.g.\n",
    "            \n",
    "            df.loc[index, 'Place.City'] = p1\n",
    "            df.loc[index, 'Place.Id'] = city_id1\n",
    "        elif not p1 in GLOBAL_CITIES:\n",
    "            if verbose:\n",
    "                print(f\"{p, p1} is unknown\")\n",
    "            omitted += row['Infected.Inc']\n",
    "df = df.drop(df.index[df['Place.Id'] < 0])\n",
    "print(f\"Omitted {omitted} cases of missing location information.\",\n",
    "      \"Total:\", df['Infected.Inc'].sum())\n",
    "\n",
    "g_days = df[\"Day.Delta\"].max() + 1\n",
    "g_all_daily_increase = np.zeros((g_all_city_num, g_days))\n",
    "for i, r in df.iterrows():\n",
    "    ci = r['Place.Id']\n",
    "    t = r['Day.Delta']\n",
    "    g_all_daily_increase[ci, t] += r['Infected.Inc']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Migration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Estimate Unit in Baidu Migration Index\n",
    "mg_df = pd.read_csv(MIGRATION_INDEX_FILENAME)\n",
    "total_out_index = mg_df.loc[9:20][\"Out\"].sum()\n",
    "total_out_number = 4096800\n",
    "num_per_index = total_out_number / total_out_index\n",
    "print(\"Per Index:\", num_per_index)\n",
    "\n",
    "# Day[0]: 1 Jan 2020\n",
    "net_out_bound = {\n",
    "    i: (row['Out'] - row['In']) * num_per_index\n",
    "    for i, row in mg_df.iterrows()\n",
    "}\n",
    "\n",
    "# Get Destination Proportion\n",
    "dest_df = pd.read_csv(MIGRATION_DEST_FILENAME)\n",
    "dest_df['Place.Id'] = pd.Series(\n",
    "    [-1] * len(dest_df), \n",
    "    index=dest_df.index)\n",
    "for i, r in dest_df.iterrows():\n",
    "    ci = get_city_id(g_popu_df, r['City'])\n",
    "    if not ci is None:\n",
    "        dest_df.loc[i, 'Place.Id'] = ci\n",
    "        \n",
    "prop_vec_ = np.zeros(g_all_city_num)\n",
    "for i, r in dest_df.iterrows():\n",
    "    prop_vec_[r['Place.Id']] = r['Percent'] / 100\n",
    "\n",
    "g_trans_data_max_day = len(mg_df)\n",
    "zero_trans_mat = np.zeros((g_all_city_num, g_all_city_num))\n",
    "g_all_trans_mats = np.zeros((g_trans_data_max_day, g_all_city_num, g_all_city_num))\n",
    "for t in range(g_trans_data_max_day):\n",
    "    g_all_trans_mats[t][g_start_loc_id_in_all] = prop_vec_ * net_out_bound[t]\n",
    "\n",
    "def get_all_city_trans(t):\n",
    "    \"\"\"\n",
    "    From Wuhan to other cities at time t.\n",
    "    \n",
    "    e.g. get day-15, net-outbound number from Wuhan.\n",
    "    get_city_trans(15)[7]\n",
    "    (7 is the CityID of Wuhan)\n",
    "    :param t: time\n",
    "    \n",
    "    NOTE: mutual-comm needs to be considered in later stage\n",
    "    \"\"\"\n",
    "\n",
    "    if t < 0 or t >= 22.99:\n",
    "        return zero_trans_mat\n",
    "    else:\n",
    "        return g_all_trans_mats[int(t)]\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Select a subset of cities for analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tmp_ids = dest_df['Place.Id'].tolist()\n",
    "g_subset_place_ids = []\n",
    "for i, p in enumerate(g_all_popu_vec):\n",
    "    if p > OPTIONS['POPULATION_THRESHOLD'] and i in tmp_ids:\n",
    "        g_subset_place_ids.append(i)\n",
    "g_subset_place_ids.append(g_start_loc_id_in_all)\n",
    "g_subset_place_ids = np.array(sorted(g_subset_place_ids), dtype=int)\n",
    "\n",
    "\n",
    "g_daily_increase = g_all_daily_increase[g_subset_place_ids]\n",
    "g_start_loc_id = np.nonzero(g_subset_place_ids == g_start_loc_id_in_all)[0][0]\n",
    "g_city_num = len(g_subset_place_ids)\n",
    "\n",
    "g_popu_vec = g_all_popu_vec[g_subset_place_ids]\n",
    "g_trans_mats = np.zeros((g_trans_data_max_day, g_city_num, g_city_num))\n",
    "for t in range(g_trans_data_max_day):\n",
    "    g_trans_mats[t][:, :] = g_all_trans_mats[t][g_subset_place_ids][:, g_subset_place_ids]\n",
    "def get_city_trans(t):\n",
    "    \"\"\"\n",
    "    From Wuhan to other cities under investigation at time t.\n",
    "    \"\"\"\n",
    "    if t < 0 or t >= 22.99:\n",
    "        return np.zeros((g_city_num, g_city_num))\n",
    "    else:\n",
    "        return g_trans_mats[int(t)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Define Networ SEIR Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NetworkSEIR:\n",
    "    def __init__(self, log_alpha, log_beta, log_gamma, \n",
    "                 log_init_infected, \n",
    "                 start_loc_id,\n",
    "                 population, tr_fn):\n",
    "        \"\"\"\n",
    "        From Wu et al., the parameters are initialised as \n",
    "\n",
    "        :param log_alpha: log of alpha: esposed -> infected\n",
    "        :param log_beta: log: susceptible -> exposed\n",
    "        :param log_gamma: log: infected -> recover\n",
    "        :param init_infected: wuhan number @ day0\n",
    "        :param start_loc_id: id of the location, e.g. wuhan\n",
    "        :param population: a vector of population in each place\n",
    "        :param tr_fn: function with signature \n",
    "          tr_fn(time):return #.cities x #.cities matrix.\n",
    "          return_mat[i, j]number of people travelling from city-i\n",
    "          to city-j\n",
    "        \"\"\"\n",
    "        self.log_alpha = log_alpha\n",
    "        self.log_beta = log_beta\n",
    "        self.log_gamma = log_gamma\n",
    "        self.log_init_infected = log_init_infected\n",
    "        \n",
    "        self.population = population\n",
    "        self.start_loc_id = start_loc_id\n",
    "        \n",
    "        self.tr_fn = tr_fn\n",
    "        self.place_n = len(population)\n",
    "\n",
    "    def diff(self, t, y):\n",
    "        alpha = np.exp(self.log_alpha)\n",
    "        beta = np.exp(self.log_beta)\n",
    "        gamma = np.exp(self.log_gamma)\n",
    "        tr_mat = self.tr_fn(t)\n",
    "        \n",
    "        ymat = y.reshape(4, self.place_n)\n",
    "        \n",
    "        S = ymat[0]\n",
    "        E = ymat[1]\n",
    "        I = ymat[2]\n",
    "        R = ymat[3]\n",
    "        N = self.population\n",
    "        K = self.tr_fn(t)\n",
    "        \n",
    "        # K.T / N : K.T[:, 7]:out num from Wuhan, N[7], popu of Wuhan\n",
    "        d = beta * (np.dot(K.T / N, I) + I) / N\n",
    "        dS = - d * S\n",
    "        dE = d * S - alpha * E\n",
    "        dI = alpha * E - gamma * I\n",
    "        dR = gamma * I\n",
    "\n",
    "        ret = np.concatenate([dS, dE, dI, dR])\n",
    "        return ret\n",
    "\n",
    "\n",
    "    def get_init_state(self):\n",
    "        init_infected = np.exp(self.log_init_infected)\n",
    "        init_exposed = init_infected / np.exp(self.log_alpha)\n",
    "        \n",
    "        v_init_infected = np.zeros(self.place_n)\n",
    "        v_init_exposed = np.zeros(self.place_n)\n",
    "        v_init_infected[self.start_loc_id] = init_infected\n",
    "        v_init_exposed = v_init_infected / np.exp(self.log_alpha)\n",
    "        v_init_suscept = self.population \\\n",
    "            - v_init_exposed - v_init_infected\n",
    "        v_init_recov = np.zeros(self.place_n)\n",
    "        \n",
    "        init_state = np.concatenate([\n",
    "            v_init_suscept,\n",
    "            v_init_exposed,\n",
    "            v_init_infected,\n",
    "            v_init_recov\n",
    "        ])\n",
    "        return init_state\n",
    "        \n",
    "    def integrate(self, t_eval, method=\"RK45\"):\n",
    "        t_end = t_eval.max() # the end of period to run ODE\n",
    "        \n",
    "        sol = solve_ivp(\n",
    "            self.diff, [0, t_end],\n",
    "            self.get_init_state(), t_eval=t_eval, method=method)\n",
    "        return dict(S=sol.y[0:self.place_n, :],\n",
    "                    E=sol.y[self.place_n:2*self.place_n, :],\n",
    "                    I=sol.y[2*self.place_n:3*self.place_n, :],\n",
    "                    R=sol.y[3*self.place_n:4*self.place_n, :]), sol\n",
    "\n",
    "\n",
    "    def neglog_likelihood(self, \n",
    "                          infection_inc_data,\n",
    "                          return_cdf=False):\n",
    "        \"\"\"\n",
    "        :param infection_inc_data: [place_num x T] of infection number \n",
    "          records.\n",
    "        :param return_cdf: if True compute CDF\n",
    "        :return: \n",
    "            negative-log-pmf\n",
    "            log-cdf: can be used to judge if an obsevation is within confidence \n",
    "              interval\n",
    "        \"\"\"\n",
    "        T = infection_inc_data.shape[1]\n",
    "        res, full_solu = self.integrate(np.arange(0, T))\n",
    "        \n",
    "        expected_infected = res[\"I\"]\n",
    "        expected_increase = np.maximum(\n",
    "            expected_infected[:, 1:] - expected_infected[:, :-1],\n",
    "            1e-9)\n",
    "        \n",
    "        real_increase = infection_inc_data[:, 1:].astype(int)\n",
    "        \n",
    "        nll = - poisson.logpmf(real_increase, expected_increase)\n",
    "        log_cdf = poisson.logcdf(real_increase, expected_increase) \\\n",
    "            if return_cdf else None\n",
    "        return nll, log_cdf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# RANSAC Model Fitting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "OPTIONS[\"ROUNDS\"] = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#=============================================================\n",
    "# Model fitting\n",
    "def regulariser(params):\n",
    "    \"\"\"\n",
    "    :param params: optim variables\n",
    "    To replace penalties like\n",
    "    pen2 = np.maximum(lg - np.log(1/2), 0)  # if gamma > 1/2\n",
    "    pen3 = np.maximum(np.log(1/1000) - lb, 0) # if beta < 1/1000\n",
    "    pen4 = np.maximum(lb - np.log(10), 0) # if beta > 10\n",
    "    \"\"\"\n",
    "    \n",
    "    \n",
    "    lb, lg, I0 = params\n",
    "    \n",
    "    param_values = [lb, lb, lg, lg, I0, I0]\n",
    "    regu_bound_keys = ['small_beta', 'large_beta', \n",
    "                       'small_gamma', 'large_gamma',\n",
    "                       'small_I0', 'large_I0']\n",
    "    pen = 0\n",
    "    for pv, rk in zip(param_values, regu_bound_keys):\n",
    "        rv = np.log(OPTIONS['REGULARISE'][rk])\n",
    "        w = OPTIONS['REGULARISE'][rk+\"_k\"]\n",
    "        if rk.startswith(\"small\"): \n",
    "            # penalise if pv < rv\n",
    "            pen += np.maximum(rv - pv, 0) * w\n",
    "        elif rk.startswith(\"large\"): \n",
    "            pen += np.maximum(pv - rv, 0) * w\n",
    "    \n",
    "    return pen\n",
    "\n",
    "def obj_fn(params, \n",
    "           inc_data, observ_mask,\n",
    "           alpha, start_id, popu_vec, tr_fn):\n",
    "    \"\"\"\n",
    "    :param place_mask: [place_num x T] vector. Counting on limited evidences\n",
    "    \"\"\"\n",
    "    \n",
    "    lb, lg, linfect0 = params\n",
    "    \n",
    "    seir_model = NetworkSEIR(\n",
    "        np.log(alpha), \n",
    "        lb, lg, linfect0, start_id,\n",
    "        popu_vec, tr_fn)\n",
    "    nll, _ = seir_model.neglog_likelihood(infection_inc_data=inc_data)\n",
    "    \n",
    "    nll *= observ_mask\n",
    "    nlls = nll.sum()\n",
    "    s = observ_mask.sum()\n",
    "    \n",
    "    obj_val = nlls + regulariser(params) * s\n",
    "    return obj_val\n",
    "\n",
    "def fit_model(obj_fn, daily_increase_data, observ_mask):\n",
    "    \"\"\"\n",
    "    This function depends on global variables\n",
    "    \"\"\"\n",
    "    beta0 = OPTIONS['INIT_BETA']\n",
    "    gamma0 = OPTIONS['INIT_GAMMA']\n",
    "    infect0_guess = OPTIONS['INIT_INIT_INFECT']\n",
    "    al = OPTIONS['ALPHA']\n",
    "    msol = minimize(\n",
    "        obj_fn, [np.log(beta0), np.log(gamma0), np.log(infect0_guess)],\n",
    "        args=(daily_increase_data,\n",
    "              observ_mask,\n",
    "              al,\n",
    "              g_start_loc_id,\n",
    "              g_popu_vec,\n",
    "              get_city_trans),\n",
    "        method='Nelder-Mead', options=dict(disp=False))\n",
    "    \n",
    "    lb, lg, i0 = msol.x\n",
    "    seir_model = NetworkSEIR(\n",
    "        np.log(al), lb, lg, i0, \n",
    "        g_start_loc_id,\n",
    "        g_popu_vec,\n",
    "        get_city_trans)\n",
    "    \n",
    "    cdf = np.exp(seir_model.neglog_likelihood(\n",
    "        g_daily_increase, True)[1])\n",
    "    is_inlier = np.logical_and(cdf > 0.05, cdf < 0.95)\n",
    "    inlier_num = is_inlier\n",
    "    \n",
    "    simu_resu, sol = seir_model.integrate(\n",
    "        np.arange(0, OPTIONS['SIMULATION_DAYS']))\n",
    "    \n",
    "    return msol, is_inlier, seir_model, simu_resu\n",
    "\n",
    "resu = dict(city_id=[],\n",
    "            days=[],\n",
    "            param_sol=[],\n",
    "            fit_num=[])\n",
    "\n",
    "rng = np.random.RandomState(0)\n",
    "best_inlier_num = -1\n",
    "g_ransac_succ = False\n",
    "for epoch in range(OPTIONS['ROUNDS']):\n",
    "    obs_city_id = rng.randint(g_city_num)\n",
    "    obs_days = np.sort(rng.choice(\n",
    "        np.arange(20, g_daily_increase.shape[1] - 1), \n",
    "        size=(OPTIONS['RANSAC_SAMPLE_NUM'],), \n",
    "        replace=False))\n",
    "    \n",
    "    observ_mask = np.zeros((g_city_num, g_daily_increase.shape[1]-1))\n",
    "    observ_mask[obs_city_id][obs_days] = 1\n",
    "    \n",
    "    msol, is_inlier, seir_model, simu_resu = fit_model(\n",
    "        obj_fn, g_daily_increase, observ_mask)\n",
    "    \n",
    "    lb, lg, i0 = msol.x\n",
    "    beta = np.exp(lb)\n",
    "    gamma = np.exp(lg)\n",
    "    R0 = beta / gamma\n",
    "    \n",
    "    #simu_resu, sol = seir_model.integrate(np.arange(0, 90))\n",
    "    \n",
    "    inlier_num = is_inlier.sum()\n",
    "    rd_ = - OPTIONS['RECENT_INLIER_DAYS']\n",
    "    recent_inlier_num = is_inlier[:, rd_:].sum()\n",
    "    \n",
    "    \n",
    "    resu['city_id'].append(obs_city_id)\n",
    "    resu['days'].append(obs_days)\n",
    "    resu['param_sol'].append(msol.x)\n",
    "    resu['fit_num'].append((inlier_num, recent_inlier_num))\n",
    "    \n",
    "    if inlier_num > best_inlier_num:\n",
    "        best_inlier_num = inlier_num\n",
    "        best_model = seir_model\n",
    "        best_simu = simu_resu\n",
    "    \n",
    "    print(f\"[{epoch}] beta: {np.exp(lb):.3f}, gamma: {np.exp(lg):.3f}, \" + \n",
    "          f\"I0: {np.exp(i0):.1f}, R0: {R0:.2f}, fit:{inlier_num}/{recent_inlier_num}:{best_inlier_num}\")\n",
    "g_ransac_succ = True    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# estimation with I0 upbound 1000, times 1000, fit-city=1, fit-days=4\n",
    "if g_ransac_succ:\n",
    "    with open(CHECKPOINT_FILENAME + \".json\", \"w\") as f:\n",
    "        json.dump(resu, f, indent=2, cls=NpEncoder)\n",
    "    with open(CHECKPOINT_FILENAME + \"_options.json\", \"w\") as f:\n",
    "        json.dump(OPTIONS, f, indent=2, cls=NpEncoder)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Graphical Report of Results - Part 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Estimated parameters and Make predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(CHECKPOINT_FILENAME + \".json\", \"r\") as f:\n",
    "    resu = json.load(f)\n",
    "# summarise results to report\n",
    "d_ = [[np.exp(ps[0]), np.exp(ps[1]), np.exp(ps[2]), \n",
    "       np.exp(ps[0]-ps[1])] + fn \n",
    "      for ps, fn in zip(resu['param_sol'], resu['fit_num'])]\n",
    "g_resu_df = pd.DataFrame(\n",
    "    data=d_, \n",
    "    columns=['beta', 'gamma', 'I0', 'R0', 'fit', 'recent_fit'])\\\n",
    "    .sort_values(by=['recent_fit'], ascending=False)\n",
    "\n",
    "# Perform prediction using the optimal model\n",
    "# an old set of param also good\n",
    "# lb, lg, li0 = np.log([0.662, 0.117, 964.0])\n",
    "g_resu_optm = g_resu_df.iloc[0]\n",
    "seir_optm = NetworkSEIR(\n",
    "    np.log(OPTIONS['ALPHA']),\n",
    "    np.log(g_resu_optm['beta']),\n",
    "    np.log(g_resu_optm['gamma']),\n",
    "    np.log(g_resu_optm['I0']),\n",
    "    g_start_loc_id,\n",
    "    g_popu_vec,\n",
    "    get_city_trans)\n",
    "simu_resu, sol = seir_optm.integrate(np.arange(0, 365))\n",
    "sim_I = simu_resu[\"I\"]\n",
    "\n",
    "accu_data = np.cumsum(g_daily_increase, axis=1)\n",
    "time_to_pred = 45\n",
    "time_with_data = accu_data.shape[1]\n",
    "time_points_sim = DAY_LIWENLIANG \\\n",
    "    + np.timedelta64(1, 'D') * np.arange(0, time_to_pred)\n",
    "time_points_data = time_points_sim[:time_with_data]\n",
    "\n",
    "nlogpmf, logcdf = seir_optm.neglog_likelihood(\n",
    "    g_daily_increase, True)\n",
    "cdf = np.exp(logcdf)\n",
    "inlier_status = np.zeros((g_city_num, time_with_data - 1)).astype(np.int)\n",
    "inlier_status[cdf < 0.05] = -1\n",
    "inlier_status[cdf > 0.95] = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(g_resu_optm, 85*5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Draw Estimated Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = px.scatter(\n",
    "    g_resu_df, x='I0', y='R0', \n",
    "    color='recent_fit', width=800, height=600)\\\n",
    "    .add_trace(go.Scatter(\n",
    "        x=[g_resu_optm['I0']], y=[g_resu_optm['R0']], \n",
    "        marker=dict(symbol='triangle-up', size=24, \n",
    "                    line_color='black', line_width=3,\n",
    "                    color='rgba(0, 255, 0, 0.5)'),\n",
    "        showlegend=False))\\\n",
    "    .update_layout(\n",
    "        coloraxis_colorbar=dict(\n",
    "            title=dict(text=\"Number of Inliers<br>(Recent 5D)\",\n",
    "                       font_size=16),\n",
    "            thicknessmode=\"pixels\", thickness=50,\n",
    "            yanchor=\"top\", y=1,\n",
    "            ticks=\"inside\",\n",
    "        dtick=20),\n",
    "        yaxis=go.layout.YAxis(\n",
    "            ticks='inside',\n",
    "            tickfont=dict(size=14),\n",
    "            title=dict(text=\"Basic Reproductive Number R0\",\n",
    "                       font_size=24),\n",
    "            range=[0, 30]),\n",
    "        xaxis=go.layout.XAxis(\n",
    "            showticklabels=True,\n",
    "            ticks='inside',\n",
    "            tickfont=dict(size=16),\n",
    "            type='log',\n",
    "            title=dict(text=\"Infections on 1 Jan 2020\",\n",
    "                       font_size=24),\n",
    "            range=[0, 4]),\n",
    "        shapes=[\n",
    "            # unfilled circle\n",
    "            go.layout.Shape(\n",
    "                type=\"circle\",\n",
    "                xref=\"x\",\n",
    "                yref=\"y\",\n",
    "                x0=80,\n",
    "                y0=6,\n",
    "                x1=240,\n",
    "                y1=14,\n",
    "                line=dict(color=\"rgb(0, 128, 0)\",\n",
    "                          width=3, dash='dash')),\n",
    "            go.layout.Shape(\n",
    "                type=\"circle\",\n",
    "                xref=\"x\",\n",
    "                yref=\"y\",\n",
    "                x0=400,\n",
    "                y0=2.5,\n",
    "                x1=1200,\n",
    "                y1=7,\n",
    "                line=dict(color=\"rgb(0, 128, 0)\",\n",
    "                          width=3, dash='dash'),\n",
    "            )],\n",
    "        annotations=[\n",
    "            go.layout.Annotation(\n",
    "                text='1',\n",
    "                font=dict(\n",
    "                    size=32,\n",
    "                    color=\"rgb(0, 128, 0)\"),\n",
    "                align='left',\n",
    "                showarrow=False,\n",
    "                xref='x',\n",
    "                yref='y',\n",
    "                x=2,\n",
    "                y=14.5,\n",
    "                bordercolor=None,),\n",
    "            go.layout.Annotation(\n",
    "                text='2',\n",
    "                font=dict(\n",
    "                    size=32,\n",
    "                    color=\"rgb(0, 128, 0)\"),\n",
    "                align='left',\n",
    "                showarrow=False,\n",
    "                xref='x',\n",
    "                yref='y',\n",
    "                x=3.05,\n",
    "                y=2,\n",
    "                bordercolor=None,)\n",
    "        ])\n",
    "\n",
    "fig.show()\n",
    "with open (\"checkpoints/fig1a_param_lscale.pdf\", 'wb') as f:\n",
    "    f.write(fig.to_image(\"pdf\", 800, 600))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig=fig.update_xaxes(range=[np.log10(10), np.log10(500)])\\\n",
    "    .update_yaxes(range=[5, 15], title=None)\\\n",
    "    .update_xaxes(range=[np.log10(80), np.log10(200)])\\\n",
    "    .update_layout(coloraxis=dict(showscale=False),\n",
    "                   height=600, width=600)\n",
    "fig.show()\n",
    "with open (\"checkpoints/fig1a_param_zoomed.pdf\", 'wb') as f:\n",
    "    f.write(fig.to_image(\"pdf\", 600, 600))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Draw prediction at different places"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Eng City Name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# quick dirty patch!\n",
    "g_eng_city_name={\"重庆市\":\"Chongqing\",\n",
    "\"上海市\":\"Shanghai\",\n",
    "\"北京市\":\"Beijing\",\n",
    "\"成都市\":\"Chengdu\",\n",
    "\"天津市\":\"Tianjin\",\n",
    "\"广州市\":\"Guangzhou\",\n",
    "\"深圳市\":\"Shenzhen\",\n",
    "\"武汉市\":\"Wuhan\",\n",
    "\"南阳市\":\"Nanyang\",\n",
    "\"临沂市\":\"Linyi\",\n",
    "\"石家庄市\":\"Shijiazhuang\",\n",
    "\"哈尔滨市\":\"Harbin\",\n",
    "\"苏州市\":\"Suzhou\",\n",
    "\"保定市\":\"Baoding\",\n",
    "\"郑州市\":\"Zhengzhou\",\n",
    "\"西安市\":\"Xi'an\",\n",
    "\"邯郸市\":\"Handan\",\n",
    "\"温州市\":\"Wenzhou\",\n",
    "\"周口市\":\"Zhoukou\",\n",
    "\"杭州市\":\"Hangzhou\",\n",
    "\"徐州市\":\"Xuzhou\",\n",
    "\"赣州市\":\"Ganzhou\",\n",
    "\"菏泽市\":\"Heze\",\n",
    "\"东莞市\":\"Dongguan\",\n",
    "\"泉州市\":\"Quanzhou\",\n",
    "\"南京市\":\"Nanjing\",\n",
    "\"阜阳市\":\"Fuyang\",\n",
    "\"商丘市\":\"Shangqiu\",\n",
    "\"南通市\":\"Nantong\",\n",
    "\"盐城市\":\"Yancheng\",\n",
    "\"驻马店市\":\"Zhumadian\",\n",
    "\"衡阳市\":\"Hengyang\",\n",
    "\"沧州市\":\"Cangzhou\",\n",
    "\"福州市\":\"Fuzhou\",\n",
    "\"邢台市\":\"Xingtai\",\n",
    "\"邵阳市\":\"Shaoyang\",\n",
    "\"长沙市\":\"Changsha\",\n",
    "\"湛江市\":\"Zhanjiang\",\n",
    "\"南宁市\":\"Nanning\",\n",
    "\"黄冈市\":\"Huanggang\",\n",
    "\"南充市\":\"Nanyun\",\n",
    "\"洛阳市\":\"Luoyang\",\n",
    "\"上饶市\":\"Shangrao\",\n",
    "\"昆明市\":\"Kunming\",\n",
    "\"无锡市\":\"Wuxi\",\n",
    "\"信阳市\":\"Xinyang\",\n",
    "\"台州市\":\"Taizhou\",\n",
    "\"常德市\":\"Changde\",\n",
    "\"新乡市\":\"Xinxiang\",\n",
    "\"合肥市\":\"Hefei\",\n",
    "\"荆州市\":\"Jingzhou\",\n",
    "\"六安市\":\"Liuan\",\n",
    "\"襄阳市\":\"Xiangyang\",\n",
    "\"岳阳市\":\"Yueyang\",\n",
    "\"达州市\":\"Dazhou\",\n",
    "\"宜春市\":\"Yichun\",\n",
    "\"宿州市\":\"Suzhou\",\n",
    "\"安庆市\":\"Anqing\",\n",
    "\"永州市\":\"Yongzhou\",\n",
    "\"安阳市\":\"Anyang\",\n",
    "\"南昌市\":\"Nanchang\",\n",
    "\"平顶山市\":\"Pingdingshan\",\n",
    "\"亳州市\":\"Haozhou\",\n",
    "\"孝感市\":\"Xiaogan\",\n",
    "\"吉安市\":\"Ji'an\",\n",
    "\"桂林市\":\"Guilin\",\n",
    "\"怀化市\":\"Huaihua\",\n",
    "\"九江市\":\"Jiujiang\",\n",
    "\"开封市\":\"Kaifeng\",\n",
    "\"泰州市\":\"Taizhou\",\n",
    "\"惠州市\":\"Huizhou\",\n",
    "\"郴州市\":\"Binzhou\",\n",
    "\"扬州市\":\"Yangzhou\",\n",
    "\"益阳市\":\"Yiyang\",\n",
    "\"许昌市\":\"Xuchang\",\n",
    "\"宜昌市\":\"Yichang\",\n",
    "\"抚州市\":\"Fuzhou\",\n",
    "\"株洲市\":\"Zhuzhou\",\n",
    "\"娄底市\":\"Loudi\",\n",
    "\"湘潭市\":\"Xiangtan\",\n",
    "\"濮阳市\":\"Puyang\",\n",
    "\"焦作市\":\"Jiaozuo\",\n",
    "\"厦门市\":\"Xiamen\",\n",
    "\"十堰市\":\"Shiyan\",\n",
    "\"恩施州\":\"Enshi\"}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Draw Figure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "figures = []\n",
    "show_dates = True\n",
    "\n",
    "def draw_city_prediction(city_id):\n",
    "    city_name = g_popu_df.loc[g_subset_place_ids[city_id], 'City']\n",
    "    predicted_infection_numbers = sim_I[city_id, :time_to_pred]\n",
    "    observed_infection_numbers = accu_data[city_id]\n",
    "    inlier_mark = is_inlier[city_id]\n",
    "    fig_data = [\n",
    "        go.Scatter(x=time_points_sim[10:], y=predicted_infection_numbers[10:],\n",
    "                   line_width=16, line_color='rgba(255,0,0,0.3)'),\n",
    "        go.Scatter(x=time_points_data[11:], \n",
    "                   y=observed_infection_numbers[11:], \n",
    "                   marker=dict(\n",
    "                       symbol=['triangle-down' if s_ < 0 \n",
    "                               else ('triangle-up' if s_ > 0 \n",
    "                                     else 'circle')\n",
    "                               for s_ in inlier_status[city_id, 10:]], \n",
    "                       size=18,\n",
    "                       line_width=1,\n",
    "                       color=['rgba(0, 192, 0, 1)' if s_ < 0 \n",
    "                              else ('rgba(192, 0, 0, 1)' if s_ > 0 \n",
    "                                    else 'rgba(0, 0, 192, 0.5)')\n",
    "                              for s_ in inlier_status[city_id, 10:]]),\n",
    "                   mode='markers'),\n",
    "#         go.Scatter(x=time_points_data[inlier_mark], y=observed_infection_numbers[inlier_mark],\n",
    "#                    marker=dict(size=28, line_width=4, \n",
    "#                                color='rgba(0,0,255,0.2)'),\n",
    "#                    mode='markers')\n",
    "    ]\n",
    "    fig = go.Figure(data=fig_data)\\\n",
    "        .update_layout(\n",
    "            autosize=False,\n",
    "            showlegend=False,\n",
    "            width=800,\n",
    "            height=800,\n",
    "            title=dict(\n",
    "                text=city_name+\"<br>\" + g_eng_city_name[city_name],\n",
    "                y=0.9, x=0.5,\n",
    "                xanchor='center', yanchor='top',\n",
    "                font=dict(size=60)\n",
    "            ),\n",
    "            margin=dict(l=100, r=0, t=0, b=0),\n",
    "            yaxis=go.layout.YAxis(\n",
    "                ticks='inside',\n",
    "                tickfont=dict(size=40)\n",
    "            ),\n",
    "            xaxis=go.layout.XAxis(\n",
    "                showticklabels=show_dates,\n",
    "                ticks='inside',\n",
    "                tickfont=dict(size=30)\n",
    "            ))\n",
    "    return fig\n",
    "    # fname_suf = 'wd' if show_dates else ''\n",
    "    # file_name = f'checkpoints/pred_num_figures_r1/{fname_suf}{city_id:03d}.pdf'\n",
    "    # with open (file_name, 'wb') as f:\n",
    "    #     f.write(fig.to_image(\"pdf\", 800, 800))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = draw_city_prediction(0)\n",
    "fig.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": false,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
